# 6 Benchmarking

Now that we have carried out a systematic review of prompting techniques, we will analyze the empirical performance of different techniques in two ways: via a formal benchmark evaluation, and by illustrating in detail the process of prompt engineering on a challenging real-world problem.

## 6.1 Technique Benchmarking

A formal evaluation of prompting techniques might be done in a broad study that compares hundreds of them across hundreds of models and benchmarks. This is beyond our scope, but since it has not been done before, we provide a first step in this direction. We choose a subset of prompting techniques and run them on the widely used benchmark MMLU (Hendrycks et al., 2021). We ran on a representative subset of 2,800 MMLU questions (20% of the questions from each category).^13 and used gpt-3.5-turbo for all experiments.

### 6.1.1 Comparing Prompting Techniques

We benchmark six distinct prompting techniques using the same general prompt template (Figure 6.2). This template shows the location of different components of the prompts. Only base instructions and question exist in every prompt. The base instruction is a phrase like "Solve the problem and return (A), (B), (C) or (D)." that we vary in some cases. We additionally test two formats of the question (Figures 6.3 and 6.4). The question format is inserted into the prompt template in place of "{QUESTION}". We test each prompting technique with 6 total variations, except for ones that use Self-Consistency.

**Zero-Shot** As a baseline, we ran questions directly through the model without any special prompting technique, only the base instruction and question. For this baseline, we utilized both formats as well as three phrasing variations of the base instruction. Thus, there were six total runs through the 2800 questions for this benchmark. This did not include any exemplars or thought inducers.

**Zero-Shot-CoT Techniques** We ran also ran Zero-Shot-CoT. As the three different variations, we used three thought inducers (instructions that cause the model to generate reasoning steps) including the standard "Let's think step by step" chain-of-thought (Kojima et al., 2022), as well as ThoT (Zhou et al., 2023), and Plan and Solve (Wang et al., 2023f). Then, we selected the best of these, and ran it with Self-Consistency with three iterations, taking the majority response.

**Few-Shot Setups** We also ran Few-Shot prompts and Few-Shot-CoT prompts, both with exemplars generated by one of our authors. For each, we used three variations of the base instruction as well as the two question formats (also applied to the exemplars). Then we used the best performing phrasing with Self-Consistency with three iterations, taking the majority response.

![Figure 6.1: Accuracy values are shown for each prompting technique, with the model used being gpt-3.5-turbo. Purple error bars illustrate the minimum and maximum for each technique, since they were each run on different phrasings and formats (except SC).](https://via.placeholder.com/600x400?text=Figure+6.1+Accuracy+Chart)

### 6.1.2 Question Formats

We experiment with two formatting choices from Sclar et al. (2023b), who explored how formatting choices can affect benchmarking results. We use two formats which lead to varied results on their task (Figures 6.3 and 6.4).

### 6.1.3 Self-Consistency

For the two Self-Consistency results, we set temperature to 0.5, following Wang et al. (2022)'s guidelines.^13 For all other prompts, a temperature of 0 was used.

^13 We excluded human_sexuality, since gpt-3.5-turbo refused to answer these questions.

```
{BASE_INSTRUCTION}
{EXEMPLARS}
{QUESTION} {THOUGHT_INDUCER}
```

**Figure 6.2:** Prompt template for benchmarking.

**Problem**
{QUESTION}

**Options**
(A)::{A} (B)::{B} (C)::{C} (D)::{D}

**Answer**

**Figure 6.3:** Question format 1.

**PROBLEM::{QUESTION}, OPTIONS::**
(A): {A}
(B): {B}
(C): {C}
(D): {D}, ANSWER::

**Figure 6.4:** Question format 2.

### 6.1.4 Evaluating Responses

Evaluating whether a LLM has properly responded to a question is a difficult task (Section 2.5). We marked answers as correct if they followed certain identifiable patterns, such as being the only capitalized letter (A-D) within parentheses or following a phrase like "The correct answer is".

### 6.1.5 Results

Performance generally improved as techniques grew more complex (Figure 6.1). However, Zero-Shot-CoT dropped precipitously from Zero-Shot. Although it had a wide spread, for all variants, Zero-Shot performed better. Both cases of Self-Consistency, naturally had lower spread since they repeated a single technique, but it only improved accuracy for Zero-Shot prompts. Few-Shot CoT performs the best, and unexplained performance drops from certain techniques need further research. As prompting technique selection is akin to hyperparameter search, this it is a very difficult task (Khattab et al., 2023). However, we hope this small study spurs research in the direction of more performant and robust prompting techniques.

## 6.2 Prompt Engineering Case Study

Prompt engineering is emerging as an art that many people have begun to practice professionally, but the literature does not yet include detailed guidance on the process. As a first step in this direction, we present an annotated prompt engineering case study for a difficult real-world problem. This is not intended to be an empirical contribution in terms of actually solving the problem. Rather, it provides one illustration of how an experienced prompt engineer would approach a task like this, along with lessons learned.

### 6.2.1 Problem

Our illustrative problem involves detection of signal that is predictive of crisis-level suicide risk in text written by a potentially suicidal individual. Suicide is a severe problem worldwide, compounded, as are most mental health issues, by a desperate lack of mental health resources. In the United States, more than half the national population lives in federally defined mental heath provider shortage areas (National Center for Health Workforce Analysis, 2023); in addition, many mental health professionals lack core competencies in suicide prevention (Cramer et al., 2023). In 2021, 12.3M Americans thought seriously about suicide, with 1.7M actually making attempts resulting in over 48,000 deaths (CDC, 2023). In the U.S., suicide was the second leading cause of death (after accidents) in people aged 10-14, 15-24, or 25-34 as of 2021 statistics, and it was the fifth leading cause of death in people aged 35–54 (Garnett and Curtin, 2023).

Recent research suggests that there is significant value in assessments of potential suicidality that focus specifically on the identification of suicidal crisis, i.e. the state of acute distress associated with a high risk of imminent suicidal behavior. However, validated assessments for diagnostic approaches such as Suicide Crisis Syndrome (SCS) (Schuck et al., 2019b; Melzer et al., 2024) and Acute Suicidal Affective Disturbance (Rogers et al., 2019) require either personal clinical interactions or completion of self-report questionnaires that contain dozens of questions. The ability to accurately flag indicators of suicidal crisis in individuals' language could therefore have a large impact within the mental health ecosystem, not as a replacement for clinical judgment but as a way to complement existing practices (Resnik et al., 2021).

As a starting point, we focus here on the most important predictive factor in Suicide Crisis Syndrome assessments, referred to in the literature as either frantic hopelessness or entrapment, "a desire to escape from an unbearable situation, tied with the perception that all escape routes are blocked" (Melzer et al., 2024).^14 This characteristic of what an individual is experiencing is also central in other characterizations of mental processes that result in suicide.

### 6.2.2 The Dataset

We worked with a subset of data from the University of Maryland Reddit Suicidality Dataset (Shing et al., 2018), which is constructed from posts in r/SuicideWatch, a subreddit that offers peer support for anyone struggling with suicidal thoughts. Two coders trained on the recognition of the factors in Suicide Crisis Syndrome coded a set of 221 posts for presence or absence of entrapment, achieving solid inter-coder reliability (Krippendorff's alpha = 0.72).

### 6.2.3 The Process

An expert prompt engineer, who has authored a widely used guide on prompting (Schulhoff, 2022), took on the task of using an LLM to identify entrapment in posts.^15 The prompt engineer was given a brief verbal and written summary of Suicide Crisis Syndrome and entrapment, along with 121 development posts and their positive/negative labels (where "positive" means entrapment is present), the other 100 labeled posts being reserved for testing. This limited information mirrors frequent real-life scenarios in which prompts are developed based on a task description and the data. More generally, it is consistent with a tendency in natural language processing and AI more generally to approach coding (annotation) as a labeling task without delving very deeply into the fact that the labels may, in fact, refer to nuanced and complex underlying social science constructs.

We documented the prompt engineering process in order to illustrate the way that an experienced prompt engineer goes about their work. The exercise proceeded through 47 recorded development steps, cumulatively about 20 hours of work. From a cold start with 0% performance (the prompt wouldn't return properly structured responses), performance was boosted to an F1 of 0.53, where that F1 is the harmonic mean of 0.86 precision and 0.38 recall.^16

Below, the set of prompts *qinf* is the test item, while *qi*, *ri*, and *ai* denote the questions, chain-of-thought steps, and answers in exemplars.

#### 6.2.3.1 Dataset Exploration (2 steps)

The process began with the prompt engineer reviewing a description of entrapment (Figure 6.7); this description had been used as a first-pass rubric for the human coders early in the coding process, noting, however, that they were familiar with SCS and knew it was neither a formal definition nor exhaustive. The prompt engineer then loaded the dataset into a Python notebook for data exploration purposes. He began by asking gpt-4-turbo-preview if it knew what entrapment was (Figure 6.8), but found that the LLM's response was not similar to the description that had been given. In consequence, the prompt engineer included the Figure 6.7 description of entrapment in all future prompts.

#### 6.2.3.2 Getting a Label (8 steps)

As noted in Section 6.1 with regard to the human_sexuality subset of MMLU, LLMs exhibit unpredictable and difficult to control behaviour in sensitive domains. For multiple steps in the prompt engineering process, the prompt engineer found that the LLM was giving mental health advice (e.g. Figure 6.9) instead of labeling the input. This was addressed by switching to the GPT-4-32K model.

A take-away from this initial phase is that the "guard rails" associated with some large language models may interfere with the ability to make progress on a prompting task, and this could influence the choice of model for reasons other than the LLM's potential quality.

#### 6.2.3.3 Prompting Techniques (32 steps)

The prompt engineer then spent the majority of his time improving the prompting technique being used. This included techniques such as Few-Shot, Chain-of-Thought, AutoCoT, Contrastive CoT, and multiple answer extraction techniques. We report statistics for the first runs of these techniques; F1 scores could change by as much as 0.04 upon subsequent runs, even with temperature and top p set to zero.^17

**Zero-Shot + Context** was the first technique evaluated (Figure 6.10), using the description in Figure 6.7. Notice the word definition in the prompt, although Figure 6.7 is not a formal definition. In order to obtain a final response from the LLM to use in calculating performance metrics, it was necessary to extract a label from the LLM output. The prompt engineer tested two extractors, one that checks if the output is exactly "Yes" or "No", and another which just checks if those words match the first few characters of the output. The latter had better performance, and it is used for the rest of this section until we reach CoT. This approach obtained 0.40 F1, 1.0 recall, and 0.25 precision, evaluated on all samples from the training/development since no samples had been used as exemplars.

**10-Shot + Context.** Next, the prompt engineer added the first ten data samples (with labels) into the prompt, in Q: (question) A: (answer) format (Figure 6.11). He evaluated this 10-shot prompt on the remaining items in the training/development set, yielding ↑0.05 (0.45) F1, ↓0.09 (0.91) recall, and ↑ 0.05 (0.30) precision, relative to the previous best prompt.^18

**One-Shot AutoDiCot + Full Context.** After performing 10-shot prompting, the prompt engineer observed that the 12th item in the development set was being incorrectly being labeled as a positive instance, and began experimenting with ways of modifying the prompting such that the model would get that item correct. In order to get a sense of why this mislabeling was taking place, the prompt engineer prompted the LLM to generate an explanation of why the 12th item would have been labeled the way it was.^19

^14 The former term more explicitly emphasizes the frantic and desperate action required to escape an unbearable life situation. However, the term entrapment is briefer and used widely so we adopt it here.

^15 Disclosure: that expert is also the lead author of this paper.

^16 Precision is also known as positive predictive value, and recall is also known as true positive rate or sensitivity. Although F1 is often used in computional system evaluations as a single figure of merit, we note that in this problem space its even weighting of precision and recall is probably not appropriate. We discuss this further below.

^17 Temperature and top-p are configuration hyperparameters that control randomness of the output (Schulhoff, 2022).

^18 Here and for the remainder of the case study, we judge "best" by F1, and we report on the current prompt under discussion relative to the best performing previous prompt.

^19 We are trying to avoid misleading language like "the LLM generated an explanation of its reasoning". LLMs do not have access to their own internal processes, and therefore they cannot "explain their reasoning" in the usual sense. An LLM generating an "explanation" is producing description of potential reasoning steps in getting to the output that could be true, but also may not be accurate at all.

**Entrapment:**
- Feeling like there is no exit
- Feeling hopeless
- Feeling like there is no way out
- Feeling afraid that things will never be normal again
- Feeling helpless to change
- Feeling trapped
- Feeling doomed
- Feeling or thinking that things will never change
- Feeling like there is no escape
- Feeling like there are no good solutions to problems

**Figure 6.7:** The description of entrapment used by the prompt engineer

**What is entrapment with respect to Suicide Crisis Syndrome?**

**Figure 6.8:** Question asked to the LLM to determine whether its training data had provided relevant knowledge about entrapment (it had not).

**If you're in immediate danger of harming yourself, please contact emergency services or a crisis hotline in your area. They can provide immediate support and help ensure your safety.**

**Figure 6.9:** A snippet from an output, which does not label the data point, but rather attempts to provide mental health support to the user. Such outputs are often five times as long as this snippet.

**Entrapment:**
{ENTRAPMENT DEFINITION (Figure 6.7)}
{*qinf*}
Is this entrapment? Yes or no.

**Figure 6.10:** A Zero-Shot + Context prompt, the simplest of all prompts explored in this case study.

**{ENTRAPMENT DEFINITION (Figure 6.7)}**
Q: {*q*1}
A: {*a*1}
...
Q: {*q*10}
A: {*a*10}
Q: {*qinf*}
A:

**Figure 6.11:** 10-Shot + Context Prompt

Figure 6.12 shows a version of that process, generalized to produce explanations for all development question/answer items (*qi, ai*) in a set *T* rather than just item 12. Informed by the reasoning steps *r*12 elicited with respect to the incorrectly labeled *q*12, the previous prompt was modified by including *r*12 in a One-Shot CoT example with incorrect reasoning, as an exemplar for what not to do (Figure 6.13).

We call the algorithm in Figure 6.12 Automatic Directed CoT (AutoDiCoT), since it automatically directs the CoT process to reason in a particular way. This technique can be generalized to any labeling task. It combines the automatic generation of CoTs (Zhang et al., 2022b) with showing the LLM examples of bad reasoning, as in the case of Contrastive CoT (Chia et al., 2023). The algorithm was also used in developing later prompts.

Finally, the prompt was extended with two additional pieces of context/instruction. The first was an email message the prompt engineer had received explaining overall goals of the project, which provided more context around the concept of entrapment and the reasons for wanting to label it. The second addition was inspired by the prompt engineer noticing the model was frequently over-generating a positive label for entrapment. Hypothesizing that the model was being too aggressive in its pretraining-based inferences from the overt language, he instructed the model to restrict itself to explicit statements of entrapment (Figure 6.13). Below we refer to these two pieces of context, provided in addition to the description of entrapment, as full context.

A new extractor was also used for this prompt, which checks if the last word in the output is "Yes" or "No", instead of the first word. This updated prompt was tested against all inputs in the development set except for the first 20. It did not improve F1, ↓0.09 (0.36) F1, but it led the prompt engineer in a direction that did, as discussed below. Recall dropped to ↓ 0.58 (0.33) recall and precision improved to ↑ 0.09 (0.39) precision.

At this point, though, it is worth observing that, although it did ultimately lead to a gain in F1 score, the steps taken here to cut down on over-generation of positive labels were not, in fact, the right move in terms of the longer term goals. Entrapment need not be expressed explicitly in order to be present (e.g. through phrases like "I feel trapped" or "There's no way out"); rather, clinical experts who have looked at the texts found that expressions of entrapment could be implicit and potentially quite nuanced. Moreover, in most use cases for automatically spotting entrapment in someone's language, precision and recall are unlikely to be equally important and, of the two, the recall/sensitivity (i.e. not missing people who should be flagged as at-risk) may matter more because the potential cost of a false negative is so high.

The take-away here, although the insight came later, is that it is easy for the process of prompt development to diverge from the actual goals unless regular engagement is fostered between the prompt engineer and domain experts who more deeply understand the real-world use case.

**Ablating Email.** The results of the previous changes were promising, but they did involve creating a prompt that included information from an email message that had not been created for that purpose, and which included information about the project, the dataset, etc. that were not intended for disclosure to a broad audience. Ironically, removing this email brought performance significantly down, ↓ 0.27 (0.18) F1, ↓ 0.75 (0.17) recall and ↓ 0.1 (0.20) precision. We attribute this to the fact that the email provided richer background information about the goals of the labeling. Although we would not recommend including email or any other potentially identifying information in any LLM prompt, we chose to leave the email in the prompt; this is consistent with scenarios in many typical settings, in which prompts are not expected to be exposed to others.

1. **Require:** Development items *T* with *n* pairs (*qi, ai*)
2. **For each pair** (*qi, ai*) in *T*:
   (a) Label *qi* as entrapment or not entrapment using the model
   (b) If the model labels correctly:
       i. Prompt the model with "Why?" to generate a reasoning chain *ri*
   (c) Else:
       i. Prompt the model with "It is actually [is/is not] entrapment, please explain why." to generate a reasoning chain *ri*
   (d) Store the tuple (*qi, ri, ai*)
3. **Return:** *n* tuples (*qi, ri, ai*)

**Figure 6.12:** Algorithm: Automatic Directed CoT

**{PROFESSOR'S EMAIL}**
**{ENTRAPMENT DEFINITION (Figure 6.7)}**
**IMPORTANT: Only label the post as entrapment if they explicitly say that they feel trapped.**

Q: {*q*12}
R: Although "Today I found out I have 10 days to vacate my apartment or I'll be formally evicted. I'm 2 months behind on my rent due to a bad time where I got demoted at work and rent from making roughly $1000 ever 2 weeks to around $450. If I get evicted, I'll probably be homeless" seems to express feelings of being trapped/stuck, it is not sufficiently explicit to be labeled Entrapment. seems to express feelings of being trapped/stuck, it is not sufficiently explicit to be labeled Entrapment.
A: {*a*12}
Q: {*qinf*}

**Figure 6.13:** One-Shot AutoDiCot + Full Context

**10-Shot + 1 AutoDiCoT.** As a next step, the prompt engineer tried including full context, 10 regular exemplars, and the one-shot exemplar about how not to reason. This hurt performance (Figure 6.14) ↓ 0.30 (0.15) F1, ↓ 0.08 (0.10) recall, ↓ 0.03 (0.33) precision.

**Full Context Only.** Next, a prompt was created using only full context, without any exemplars (Figure 6.15). This boosted performance over the previous technique, but did not make progress overall ↓ 0.01 (0.44) F1, ↑ 0.01 (0.92) recall, ↓ 0.01 (0.29) precision. Interestingly, in this prompt, the prompt engineer accidentally pasted in the fullcontext email twice, and that ended up having significant positive effects on performance later (and removing the duplicate actually decreased performance). This is reminiscent of the re-reading technique (Xu et al., 2023).

This can be interpreted both optimistically and pessimistically. Optimistically, it demonstrates how improvements can arise through exploration and fortuitous discovery. On the pessimistic side, the value of duplicating the email in the prompt highlights the extent to which prompting remains a difficult to explain black art, where the LLM may turn out to be unexpectedly sensitive to variations one might not expect to matter.

**10-Shot AutoDiCoT.** The next step was to create more AutoDiCoT exemplars, per the algorithm in Figure 6.12. A total of ten new AutoDiCoT exemplars were added to the full context prompt (Figure 6.16). This yielded the most successful prompt from this prompt engineering exercise, in terms of F1 score, ↑ 0.08 (0.53) F1, ↓ 0.05 (0.86) recall, ↑ 0.08 (0.38) precision.

**20-Shot AutoDiCoT.** Further experimentation proceeded seeking (unsuccesfully) to improve on the previous F1 result. In one attempt, the prompt engineer labeled an additional ten exemplars, and created a 20-shot prompt from the first 20 data points in the development set. This led to worse results than the 10-shot prompt, when tested on all samples other than the first twenty, ↓ 0.04 (0.49) F1, ↑ 0.08 (0.94) recall, ↓ 0.05 (0.33) precision. Notably, it also yielded worse performance on the test set.

**20-Shot AutoDiCoT + Full Words.** The prompt engineer conjectured that the LLM would perform better if the prompt included full words Question, Reasoning, and Answer rather than Q, R, A. However, this did not succeed (Figure 6.17), ↓ 0.05 (0.48) F1, ↑ 0.08 (0.94) recall, ↓ 0.06 (0.32) precision.

**{PROFESSOR'S EMAIL}**
**{ENTRAPMENT DEFINITION (Figure 6.7)}**
**IMPORTANT: Only label the post as entrapment if they explicitly say that they feel trapped.**

Q: {*q*1}
A: {*a*1}
...
Q: {*q*10}
A: {*a*10}
Q: {*q*12}
R: Although "{LLM REASONING}" seems to express feelings of being trapped/stuck, it is not sufficiently explicit to be labeled Entrapment.
A: {*a*12}
Q: {*qinf*}

**Figure 6.14:** 10-Shot + 1 AutoDiCoT

**{PROFESSOR'S EMAIL}**
**{ENTRAPMENT DEFINITION (Figure 6.7)}**
**IMPORTANT: Only label the post as entrapment if they explicitly say that they feel trapped.**

Q: {*qinf*} A:

**Figure 6.15:** Full Context Only

**{PROFESSOR'S EMAIL}**
**{ENTRAPMENT DEFINITION}**
**IMPORTANT: Only label the post as entrapment if they explicitly say that they feel trapped.**

Q: {*q*1}
R: {*r*1}
A: {*a*1}
...
Q: {*q*10}
R: {*r*10}
A: {*a*10}
Q: {*qinf*}

**Figure 6.16:** 10-Shot AutoDiCoT

**{PROFESSOR'S EMAIL}**
**{ENTRAPMENT DEFINITION}**
**IMPORTANT: Only label the post as entrapment if they explicitly say that they feel trapped.**

**Question:** {*q*1}
**Reasoning:** {*r*1}
**Answer:** {*a*1}
...
**Question:** {*q*20}
**Reasoning:** {*r*20}
**Answer:** {*a*20}
**Question:** {*qinf*}

**Figure 6.17:** 20-shot AutoDiCoT

**20-Shot AutoDiCoT + Full Words + Extraction Prompt.** The prompt engineer then noticed that in many cases, the LLM generated outputs that could not properly be parsed to obtain a response. So, they crafted a prompt that extracted answers from the LLM's response (Figure 6.18). Although this improved accuracy by a few points, it decreased F1, thanks to the fact that many of the outputs that had been unparsed actually contained incorrect responses, ↓ 0.05 (0.48) F1, ↓ 0.05 (0.33) precision, with no change in recall (0.86).

**10-Shot AutoDiCoT + Extraction Prompt.** Applying the extraction prompt to the best performing 10-Shot AutoDiCoT prompt did not improve results, ↓ 0.04 (0.49) F1, ↓ 0.08 (0.78) recall, ↓ 0.03 (0.35) precision.

**10-Shot AutoDiCoT without Email.** As noted above, removing the email outright from the prompt hurt performance, ↓ 0.14 (0.39) F1, ↓ 0.39 (0.48) recall, ↓ 0.06 (0.32) precision.

**De-Duplicating Email.** Also as noted above, it seemed reasonable that removing the duplication of the email would perform as well or better than the prompt with the unintentional duplication. As it turned out, however, removing the duplicate significantly hurt performance, ↓ 0.07 (0.45) F1, ↓ 0.12 (0.74) recall, ↓ 0.05 (0.33) precision.

**{PROFESSOR'S EMAIL}**
**{ENTRAPMENT DEFINITION}**
**IMPORTANT: Only label the post as entrapment if they explicitly say that they feel trapped.**

**Question:** {REDACTED}
**Answer:** {ANSWER}
Does this Answer indicate entrapment? Output the word Yes if it is labeled as entrapment and output the word No if it is not labeled as entrapment. Only output the word Yes or the word No.

**Figure 6.18:** Extraction Prompt

**10-Shot AutoDiCoT + Default to Reject.** This approach used the best performing prompt, and defaulted to labeling as negative (not entrapment) in the case of answers that are not extracted properly. This did not help performance, ↓ 0.11 (0.42) F1, ↓ 0.04 (0.83) recall, ↓ 0.10 (0.28) precision.

**Ensemble + Extraction.** Especially for systems that are sensitive to the details of their inputs, there are advantages in trying multiple variations of an input and then combining their results. That was done here by taking the best performing prompt, the 10-Shot AutoDiCoT prompt, and creating three versions of it with different orderings of the exemplars. The average of the three results was taken to be the final answer. Unfortunately, both orderings that differed from the default ordering led to the LLM not outputting a well-structured response. An extraction prompt was therefore used to obtain final answers. This exploration hurt rather than helped performance ↓ 0.16 (0.36) F1, ↓ 0.23 (0.64) recall, ↓ 0.13 (0.26) precision.

**10-Shot AutoCoT + 3x the context (no email dupe).** Recall that context refers to the description of entrapment, an instruction about explicitness, and an email. Since the duplicated email had improved performance, the prompt engineer tested out pasting in three copies of the context (first de-duplicating the email). However, this did not improve performance, ↓ 0.06 (0.47) F1, ↓ 0.08 (0.78) recall, ↓ 0.05 (0.33) precision.

**Anonymize Email.** At this point it seemed clear that including the duplicated email in the prompt was actually, although not explainably, essential to the best performance so far obtained. The prompt engineer decided to anonymize the email by replacing personal names with other, random names. However, surprisingly, this decreased performance significantly ↓ 0.08 (0.45) F1, ↓ 0.14 (0.72) recall, ↓ 0.06 (0.33) precision.

**DSPy.** We concluded the case study by exploring an alternative to manual prompt engineering, the DSPy framework (Khattab et al., 2023), which automatically optimizes LLM prompts for a given target metric. Specifically, we begin with a chain-of-thought classification pipeline that uses the definition of entrapment in Figure 6.7. Over 16 iterations, DSPy bootstrapped synthetic LLM-generated demonstrations and randomly sampled training exemplars, with the ultimate objective of maximizing *F*1 on the same development set used above. We used gpt-4-0125-preview and the default settings for the BootstrapFewShotWithRandomSearch "teleprompter" (the optimization approach). Figure 6.19 shows the results of two of these prompts on the test set, one of which used default DSPy behaviour, and the second which was manually modified slightly from this default. The best resulting prompt includes 15 exemplars (without CoT reasoning) and one bootstrapped reasoning demonstration. It achieves 0.548 *F*1 (and 0.385 / 0.952 precision / recall) on the test set, without making any use of the professor's email nor the incorrect instruction about the explicitness of entrapment. It also performs much better than the human prompt engineer's prompts on the test set, which demonstrates the significant promise of automated prompt engineering.

![Figure 6.5: F1 scores varied widely from worst performing prompts to highest performing prompts, but most prompts scored within a similar range.](https://via.placeholder.com/800x400?text=Figure+6.5+F1+Scores+Chart)

![Figure 6.6: From the first prompt tried (Zero-Shot + Context) to the last (Anonymized Email), improvements in F1 score were hard to come by and often involved testing multiple underperforming prompts before finding a performant one. Green lines show improvements over the current highest F1 score, while red lines show deteriorations.](https://via.placeholder.com/800x400?text=Figure+6.6+F1+Progress+Chart)

![Figure 6.19: Scores of different prompting techniques on the test set.](https://via.placeholder.com/600x400?text=Figure+6.19+Test+Set+Scores)

### 6.2.4 Discussion

Prompt engineering is a non-trivial process, the nuances of which are not currently well described in literature. From the fully manual process illustrated above, there are several take-aways worth summarizing. First, prompt engineering is fundamentally different from other ways of getting a computer to behave the way you want it to: these systems are being cajoled, not programmed, and, in addition to being quite sensitive to the specific LLM being used, they can be incredibly sensitive to specific details in prompts without there being any obvious reason those details should matter. Second, therefore, it is important to dig into the data (e.g. generating potential explanations for LLM "reasoning" that leads to incorrect responses). Related, the third and most important take-away is that prompt engineering should involve engagement between the prompt engineer, who has expertise in how to coax LLMs to behave in desired ways, and domain experts, who understand what those desired ways are and why.

Ultimately we found that there was significant promise in an automated method for exploring the prompting space, but also that combining that automation with human prompt engineering/revision was the most successful approach. We hope that this study will serve as a step toward more robust examinations of how to perform prompt engineering.